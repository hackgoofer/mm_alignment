{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vqa_link_train = \"/private/home/sash/mmf/mmf/v2_mscoco_train2014_annotations.json\"\n",
    "\n",
    "with open(original_vqa_link_train) as f:\n",
    "    original_vqas = json.load(f)\n",
    "    original_vqas = original_vqas[\"annotations\"] #443757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types_dict = collections.defaultdict(lambda: []) \n",
    "\n",
    "for original_vqa in original_vqas:\n",
    "    question_type = original_vqa[\"answer_type\"]\n",
    "    question_types_dict[question_type].append(original_vqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443757"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vqa_questions = question_types_dict[\"other\"] + question_types_dict[\"yes/no\"] + question_types_dict['number']\n",
    "len(sorted_vqa_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vqa_question_link_train = \"/private/home/sash/mmf/mmf/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "with open(original_vqa_question_link_train) as f:\n",
    "    original_vqa_questions = json.load(f)\n",
    "    original_vqa_questions = original_vqa_questions[\"questions\"]\n",
    "    \n",
    "question_id_to_question_text = {}\n",
    "for question in original_vqa_questions:\n",
    "    question_id_to_question_text[question[\"question_id\"]] = question[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_sampling(x, n_samples, stratify):\n",
    "    \"\"\"Perform stratify sampling of a tensor.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    x: np.ndarray or torch.Tensor\n",
    "        Array to sample from. Sampels from first dimension.\n",
    "        \n",
    "    n_samples: int\n",
    "        Number of samples to sample\n",
    "        \n",
    "    stratify: tuple of int\n",
    "        Size of each subgroup. Note that the sum of all the sizes \n",
    "        need to be equal to `x.shape[']`.\n",
    "    \"\"\"\n",
    "    n_total = x.shape[0]\n",
    "    assert sum(stratify) == n_total\n",
    "    \n",
    "    n_strat_samples = [int(i*n_samples/n_total) for i in stratify]\n",
    "    cum_n_samples = np.cumsum([0]+list(stratify))\n",
    "    sampled_idcs = []\n",
    "    for i, n_strat_sample in enumerate(n_strat_samples):\n",
    "        sampled_idcs.append(np.random.choice(range(cum_n_samples[i], cum_n_samples[i+1]), \n",
    "                                            replace=False, \n",
    "                                            size=n_strat_sample))\n",
    "        \n",
    "    # might not be correct number of samples due to rounding\n",
    "    n_current_samples = sum(n_strat_samples)\n",
    "    if  n_current_samples < n_samples:\n",
    "        delta_n_samples = n_samples - n_current_samples\n",
    "        # might actually resample same as before, but it's only for a few\n",
    "        sampled_idcs.append(np.random.choice(range(n_total), replace=False, size=delta_n_samples))\n",
    "        \n",
    "    samples = x[np.concatenate(sampled_idcs), ...]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Selected: 30\n",
      "Other Selected: 15/219269\n",
      "YesNo Selected: 12/166882\n",
      "Numbe Selected: 3/57606\n"
     ]
    }
   ],
   "source": [
    "samples = np.arange(len(sorted_vqa_questions))\n",
    "n_samples = 30\n",
    "other_len = len(question_types_dict[\"other\"])\n",
    "yesno_len = len(question_types_dict[\"yes/no\"])\n",
    "number_len = len(question_types_dict['number'])\n",
    "stratify = [other_len, yesno_len, number_len]\n",
    "output_indexes = stratify_sampling(samples, n_samples, stratify)\n",
    "\n",
    "num_selected_types = collections.defaultdict(lambda: 0)\n",
    "for output_index in output_indexes:\n",
    "    if output_index < other_len:\n",
    "        num_selected_types[\"other\"] += 1\n",
    "    elif output_index < other_len+yesno_len and output_index >= other_len:\n",
    "        num_selected_types[\"yes/no\"] += 1\n",
    "    else:\n",
    "        num_selected_types[\"number\"] += 1\n",
    "\n",
    "print(f\"Total Selected: {n_samples}\")\n",
    "print(f\"Other Selected: {num_selected_types['other']}/{other_len}\")\n",
    "print(f\"YesNo Selected: {num_selected_types['yes/no']}/{yesno_len}\")\n",
    "print(f\"Numbe Selected: {num_selected_types['number']}/{number_len}\")\n",
    "\n",
    "selected_vqa_questions = [(sorted_vqa_questions[output_index][\"question_id\"], sorted_vqa_questions[output_index][\"image_id\"]) for output_index in output_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id: 395456001\n",
      "text: What are the spots on the floor?\n",
      "image_id: 395456\n",
      "\n",
      "\n",
      "question_id: 283809004\n",
      "text: Which remote is the biggest?\n",
      "image_id: 283809\n",
      "\n",
      "\n",
      "question_id: 555586014\n",
      "text: What color is the man's tie?\n",
      "image_id: 555586\n",
      "\n",
      "\n",
      "question_id: 109816002\n",
      "text: What is the guy in black holding in his hand?\n",
      "image_id: 109816\n",
      "\n",
      "\n",
      "question_id: 474601003\n",
      "text: What gender is the birthday person?\n",
      "image_id: 474601\n",
      "\n",
      "\n",
      "question_id: 360441000\n",
      "text: What street sign is at the bottom?\n",
      "image_id: 360441\n",
      "\n",
      "\n",
      "question_id: 480890001\n",
      "text: Where is the man staring?\n",
      "image_id: 480890\n",
      "\n",
      "\n",
      "question_id: 524866016\n",
      "text: What brand is this phone?\n",
      "image_id: 524866\n",
      "\n",
      "\n",
      "question_id: 88527004\n",
      "text: What type of tie is he wearing?\n",
      "image_id: 88527\n",
      "\n",
      "\n",
      "question_id: 462512002\n",
      "text: Which way is the convertible turning?\n",
      "image_id: 462512\n",
      "\n",
      "\n",
      "question_id: 576809001\n",
      "text: Where are the orange stripes?\n",
      "image_id: 576809\n",
      "\n",
      "\n",
      "question_id: 333848003\n",
      "text: What breed is the dog?\n",
      "image_id: 333848\n",
      "\n",
      "\n",
      "question_id: 180098000\n",
      "text: Is the bridge new or old?\n",
      "image_id: 180098\n",
      "\n",
      "\n",
      "question_id: 37122004\n",
      "text: What colors make up the birds?\n",
      "image_id: 37122\n",
      "\n",
      "\n",
      "question_id: 554335003\n",
      "text: Is that US currency that he is holding?\n",
      "image_id: 554335\n",
      "\n",
      "\n",
      "question_id: 482780002\n",
      "text: Is the snow untouched?\n",
      "image_id: 482780\n",
      "\n",
      "\n",
      "question_id: 293793036\n",
      "text: Is this meal being served indoors?\n",
      "image_id: 293793\n",
      "\n",
      "\n",
      "question_id: 27227014\n",
      "text: Is this a standard sized chair?\n",
      "image_id: 27227\n",
      "\n",
      "\n",
      "question_id: 5700004\n",
      "text: Do you see a small red sign?\n",
      "image_id: 5700\n",
      "\n",
      "\n",
      "question_id: 527717000\n",
      "text: Do you think this stuffed animal is sentimental?\n",
      "image_id: 527717\n",
      "\n",
      "\n",
      "question_id: 151761010\n",
      "text: Are the elephants friends?\n",
      "image_id: 151761\n",
      "\n",
      "\n",
      "question_id: 489369002\n",
      "text: Is there a giraffe?\n",
      "image_id: 489369\n",
      "\n",
      "\n",
      "question_id: 439185033\n",
      "text: Is the man well dressed?\n",
      "image_id: 439185\n",
      "\n",
      "\n",
      "question_id: 274422000\n",
      "text: Does it appear to be winter in this photo?\n",
      "image_id: 274422\n",
      "\n",
      "\n",
      "question_id: 206787004\n",
      "text: Does that taste good?\n",
      "image_id: 206787\n",
      "\n",
      "\n",
      "question_id: 370583000\n",
      "text: How many people are visible?\n",
      "image_id: 370583\n",
      "\n",
      "\n",
      "question_id: 209374001\n",
      "text: How many types of veggies are in the image?\n",
      "image_id: 209374\n",
      "\n",
      "\n",
      "question_id: 131909019\n",
      "text: How many people are in the picture?\n",
      "image_id: 131909\n",
      "\n",
      "\n",
      "question_id: 520486004\n",
      "text: Why is the woman carrying an umbrella?\n",
      "image_id: 520486\n",
      "\n",
      "\n",
      "question_id: 430928011\n",
      "text: Is it raining?\n",
      "image_id: 430928\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question_id, image_id in selected_vqa_questions:\n",
    "    print(f\"question_id: {question_id}\")\n",
    "    print(f\"text: {question_id_to_question_text[question_id]}\")\n",
    "    print(f\"image_id: {image_id}\")\n",
    "    print(f\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
