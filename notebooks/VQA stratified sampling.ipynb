{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vqa_link_train = \"/private/home/sash/.cache/torch/mmf/data/datasets/vqa2/defaults/annotations/imdb_train2014_len_coco_50_pc.npy\"\n",
    "\n",
    "with open(original_vqa_link_train) as f:\n",
    "    original_vqas = json.load(f)\n",
    "    original_vqas = original_vqas[\"annotations\"] #443757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types_dict = collections.defaultdict(lambda: []) \n",
    "\n",
    "for original_vqa in original_vqas:\n",
    "    question_type = original_vqa[\"answer_type\"]\n",
    "    question_types_dict[question_type].append(original_vqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443757"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vqa_questions = question_types_dict[\"other\"] + question_types_dict[\"yes/no\"] + question_types_dict['number']\n",
    "len(sorted_vqa_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vqa_question_link_train = \"/private/home/sash/mmf/mmf/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "with open(original_vqa_question_link_train) as f:\n",
    "    original_vqa_questions = json.load(f)\n",
    "    original_vqa_questions = original_vqa_questions[\"questions\"]\n",
    "    \n",
    "question_id_to_question_text = {}\n",
    "for question in original_vqa_questions:\n",
    "    question_id_to_question_text[question[\"question_id\"]] = question[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_sampling(x, n_samples, stratify):\n",
    "    \"\"\"Perform stratify sampling of a tensor.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    x: np.ndarray or torch.Tensor\n",
    "        Array to sample from. Sampels from first dimension.\n",
    "        \n",
    "    n_samples: int\n",
    "        Number of samples to sample\n",
    "        \n",
    "    stratify: tuple of int\n",
    "        Size of each subgroup. Note that the sum of all the sizes \n",
    "        need to be equal to `x.shape[']`.\n",
    "    \"\"\"\n",
    "    n_total = x.shape[0]\n",
    "    assert sum(stratify) == n_total\n",
    "    \n",
    "    n_strat_samples = [int(i*n_samples/n_total) for i in stratify]\n",
    "    cum_n_samples = np.cumsum([0]+list(stratify))\n",
    "    sampled_idcs = []\n",
    "    for i, n_strat_sample in enumerate(n_strat_samples):\n",
    "        sampled_idcs.append(np.random.choice(range(cum_n_samples[i], cum_n_samples[i+1]), \n",
    "                                            replace=False, \n",
    "                                            size=n_strat_sample))\n",
    "        \n",
    "    # might not be correct number of samples due to rounding\n",
    "    n_current_samples = sum(n_strat_samples)\n",
    "    if  n_current_samples < n_samples:\n",
    "        delta_n_samples = n_samples - n_current_samples\n",
    "        # might actually resample same as before, but it's only for a few\n",
    "        sampled_idcs.append(np.random.choice(range(n_total), replace=False, size=delta_n_samples))\n",
    "        \n",
    "    samples = x[np.concatenate(sampled_idcs), ...]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Selected: 30\n",
      "Other Selected: 15/219269\n",
      "YesNo Selected: 12/166882\n",
      "Numbe Selected: 3/57606\n"
     ]
    }
   ],
   "source": [
    "samples = np.arange(len(sorted_vqa_questions))\n",
    "n_samples = 30\n",
    "other_len = len(question_types_dict[\"other\"])\n",
    "yesno_len = len(question_types_dict[\"yes/no\"])\n",
    "number_len = len(question_types_dict['number'])\n",
    "stratify = [other_len, yesno_len, number_len]\n",
    "output_indexes = stratify_sampling(samples, n_samples, stratify)\n",
    "\n",
    "num_selected_types = collections.defaultdict(lambda: 0)\n",
    "for output_index in output_indexes:\n",
    "    if output_index < other_len:\n",
    "        num_selected_types[\"other\"] += 1\n",
    "    elif output_index < other_len+yesno_len and output_index >= other_len:\n",
    "        num_selected_types[\"yes/no\"] += 1\n",
    "    else:\n",
    "        num_selected_types[\"number\"] += 1\n",
    "\n",
    "print(f\"Total Selected: {n_samples}\")\n",
    "print(f\"Other Selected: {num_selected_types['other']}/{other_len}\")\n",
    "print(f\"YesNo Selected: {num_selected_types['yes/no']}/{yesno_len}\")\n",
    "print(f\"Numbe Selected: {num_selected_types['number']}/{number_len}\")\n",
    "\n",
    "selected_vqa_questions = [(sorted_vqa_questions[output_index][\"question_id\"], sorted_vqa_questions[output_index][\"image_id\"]) for output_index in output_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id: 395456001\n",
      "text: What are the spots on the floor?\n",
      "image_id: 395456\n",
      "\n",
      "\n",
      "question_id: 283809004\n",
      "text: Which remote is the biggest?\n",
      "image_id: 283809\n",
      "\n",
      "\n",
      "question_id: 555586014\n",
      "text: What color is the man's tie?\n",
      "image_id: 555586\n",
      "\n",
      "\n",
      "question_id: 109816002\n",
      "text: What is the guy in black holding in his hand?\n",
      "image_id: 109816\n",
      "\n",
      "\n",
      "question_id: 474601003\n",
      "text: What gender is the birthday person?\n",
      "image_id: 474601\n",
      "\n",
      "\n",
      "question_id: 360441000\n",
      "text: What street sign is at the bottom?\n",
      "image_id: 360441\n",
      "\n",
      "\n",
      "question_id: 480890001\n",
      "text: Where is the man staring?\n",
      "image_id: 480890\n",
      "\n",
      "\n",
      "question_id: 524866016\n",
      "text: What brand is this phone?\n",
      "image_id: 524866\n",
      "\n",
      "\n",
      "question_id: 88527004\n",
      "text: What type of tie is he wearing?\n",
      "image_id: 88527\n",
      "\n",
      "\n",
      "question_id: 462512002\n",
      "text: Which way is the convertible turning?\n",
      "image_id: 462512\n",
      "\n",
      "\n",
      "question_id: 576809001\n",
      "text: Where are the orange stripes?\n",
      "image_id: 576809\n",
      "\n",
      "\n",
      "question_id: 333848003\n",
      "text: What breed is the dog?\n",
      "image_id: 333848\n",
      "\n",
      "\n",
      "question_id: 180098000\n",
      "text: Is the bridge new or old?\n",
      "image_id: 180098\n",
      "\n",
      "\n",
      "question_id: 37122004\n",
      "text: What colors make up the birds?\n",
      "image_id: 37122\n",
      "\n",
      "\n",
      "question_id: 554335003\n",
      "text: Is that US currency that he is holding?\n",
      "image_id: 554335\n",
      "\n",
      "\n",
      "question_id: 482780002\n",
      "text: Is the snow untouched?\n",
      "image_id: 482780\n",
      "\n",
      "\n",
      "question_id: 293793036\n",
      "text: Is this meal being served indoors?\n",
      "image_id: 293793\n",
      "\n",
      "\n",
      "question_id: 27227014\n",
      "text: Is this a standard sized chair?\n",
      "image_id: 27227\n",
      "\n",
      "\n",
      "question_id: 5700004\n",
      "text: Do you see a small red sign?\n",
      "image_id: 5700\n",
      "\n",
      "\n",
      "question_id: 527717000\n",
      "text: Do you think this stuffed animal is sentimental?\n",
      "image_id: 527717\n",
      "\n",
      "\n",
      "question_id: 151761010\n",
      "text: Are the elephants friends?\n",
      "image_id: 151761\n",
      "\n",
      "\n",
      "question_id: 489369002\n",
      "text: Is there a giraffe?\n",
      "image_id: 489369\n",
      "\n",
      "\n",
      "question_id: 439185033\n",
      "text: Is the man well dressed?\n",
      "image_id: 439185\n",
      "\n",
      "\n",
      "question_id: 274422000\n",
      "text: Does it appear to be winter in this photo?\n",
      "image_id: 274422\n",
      "\n",
      "\n",
      "question_id: 206787004\n",
      "text: Does that taste good?\n",
      "image_id: 206787\n",
      "\n",
      "\n",
      "question_id: 370583000\n",
      "text: How many people are visible?\n",
      "image_id: 370583\n",
      "\n",
      "\n",
      "question_id: 209374001\n",
      "text: How many types of veggies are in the image?\n",
      "image_id: 209374\n",
      "\n",
      "\n",
      "question_id: 131909019\n",
      "text: How many people are in the picture?\n",
      "image_id: 131909\n",
      "\n",
      "\n",
      "question_id: 520486004\n",
      "text: Why is the woman carrying an umbrella?\n",
      "image_id: 520486\n",
      "\n",
      "\n",
      "question_id: 430928011\n",
      "text: Is it raining?\n",
      "image_id: 430928\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question_id, image_id in selected_vqa_questions:\n",
    "    print(f\"question_id: {question_id}\")\n",
    "    print(f\"text: {question_id_to_question_text[question_id]}\")\n",
    "    print(f\"image_id: {image_id}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see sizes to predict cost:\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_val_qs_path = \"/private/home/sash/vqa2/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "vqa_test_qs_path = \"/private/home/sash/vqa2/v2_OpenEnded_mscoco_test2015_questions.json\"\n",
    "vqa_test_dev_qs_path = \"/private/home/sash/vqa2/v2_OpenEnded_mscoco_test-dev2015_questions.json\"\n",
    "\n",
    "\n",
    "with open(vqa_val_qs_path) as f:\n",
    "    vqa_val_qs = json.load(f)\n",
    "\n",
    "with open(vqa_test_qs_path) as f:\n",
    "    vqa_test_qs = json.load(f)\n",
    "\n",
    "with open(vqa_test_dev_qs_path) as f:\n",
    "    vqa_test_dev_qs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val len: 214354\n",
      "test len: 447793\n",
      "test-dev len: 107394\n"
     ]
    }
   ],
   "source": [
    "print(f\"val len: {len(vqa_val_qs['questions'])}\")\n",
    "print(f\"test len: {len(vqa_test_qs['questions'])}\")\n",
    "print(f\"test-dev len: {len(vqa_test_dev_qs['questions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no overlap between test and test-dev in terms of question_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340399"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test_qs_set - test_dev_qs_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340399"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "447793-107394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image height and image width\n",
    "image_info_test2015_file = \"/private/home/sash/dyna/image_info_test2015/image_info_test2015.json\"\n",
    "person_val2014_file = \"/private/home/sash/dyna/annotations_trainval2014/person_keypoints_val2014.json\"\n",
    "person_train_file = \"/private/home/sash/dyna/annotations_trainval2014/person_keypoints_train2014.json\"\n",
    "\n",
    "with open(image_info_test2015_file) as f:\n",
    "    image_info_test2015 = json.load(f)\n",
    "    \n",
    "with open(person_val2014_file) as f:\n",
    "    person_val2014 = json.load(f)\n",
    "    \n",
    "with open(person_train_file) as f:\n",
    "    person_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info_test2015 = image_info_test2015[\"images\"]\n",
    "person_val2014 = person_val2014[\"images\"]\n",
    "person_train = person_train[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81434, 40504, 82783)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_info_test2015), len(person_val2014), len(person_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids_from_image_info_test2015 = set(image[\"coco_url\"] for image in image_info_test2015)\n",
    "image_ids_from_person_val2014 = set(image[\"coco_url\"] for image in person_val2014)\n",
    "image_ids_from_person_train = set(image[\"coco_url\"] for image in person_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_from_coco = person_val2014 + person_train + image_info_test2015\n",
    "def data_from_image(image):\n",
    "    return {\n",
    "        \"coco_url\": image[\"coco_url\"],\n",
    "        \"height\": image[\"height\"],\n",
    "        \"width\": image[\"width\"],\n",
    "    }\n",
    "\n",
    "\n",
    "person_val2014_images = []\n",
    "for image in person_val2014:\n",
    "    person_val2014_images.append(data_from_image(image))\n",
    "    \n",
    "person_train_images = []\n",
    "for image in person_train:\n",
    "    person_train_images.append(data_from_image(image))\n",
    "\n",
    "image_info_test2015_images = []\n",
    "for image in image_info_test2015:\n",
    "    image_info_test2015_images.append(data_from_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\", \"w\") as f:\n",
    "    json.dump({\"train\": person_train_images}, f)\n",
    "    \n",
    "with open(\"val.json\", \"w\") as f:\n",
    "    json.dump({\"val\": person_val2014_images}, f)\n",
    "    \n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    json.dump({\"test\": image_info_test2015_images}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.102564102564102"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_aspect_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 72)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 72)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://images.cocodataset.org/train2014/COCO_train2014_000000187714.jpg',\n",
       " 'http://images.cocodataset.org/train2014/COCO_train2014_000000363747.jpg')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_height_url, min_width_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
